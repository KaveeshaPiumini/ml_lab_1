# -*- coding: utf-8 -*-
"""190495D

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o4VMkU5r-YPIgbBR2vez4Kv6E_d4PrKO
"""

from google.colab import drive
drive.mount('/content/drive')

#import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVC
from xgboost import XGBClassifier
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_squared_error, r2_score

# file paths for the datasets
train_path = '/content/drive/MyDrive/Semester 7/3 - CS4622 - Machine Learning/Lab 1/train.csv'
valid_path = '/content/drive/MyDrive/Semester 7/3 - CS4622 - Machine Learning/Lab 1/valid.csv'
test_path = '/content/drive/MyDrive/Semester 7/3 - CS4622 - Machine Learning/Lab 1/test.csv'

# load the train dataset
train_data = pd.read_csv(train_path)
# load the valid dataset
valid_data = pd.read_csv(valid_path)
# Load the test dataset
test_data = pd.read_csv(test_path)

train_data.head()

# check for null values in train dataset
train_null_counts = train_data.isnull().sum()
print("train null counts : \n {}".format(train_null_counts))

# drop columns with null values in the target labels for train dataset
train_data = train_data.dropna(subset=train_data.columns[-4:], how='any')

# fill null values with mean in train dataset
train_data = train_data.fillna(train_data.mean())

# fill null values with mean in valid dataset
valid_data = valid_data.fillna(valid_data.mean())

# fill null values with mean in test dataset
test_data = test_data.fillna(test_data.mean())

#show processed training data
train_data.head()

# separate features and labels in train dataset
train_features = train_data.iloc[:, :-4]
train_labels = train_data.iloc[:, -4:]

# get the first label of the train dataset
train_label1 = train_labels.iloc[:,0]

# get the second label of the train dataset
train_label2 = train_labels.iloc[:,1]

# get the third label of the train dataset
train_label3 = train_labels.iloc[:,2]

# get the fourth label of the train dataset
train_label4 = train_labels.iloc[:,3]



# separate features and labels in valid dataset
valid_features = valid_data.iloc[:, :-4]
valid_labels = valid_data.iloc[:, -4:]

# get the first label of the valid dataset
valid_label1 = valid_labels.iloc[:,0]

# get the second label of the valid dataset
valid_label2 = valid_labels.iloc[:,1]

# get the third label of the valid dataset
valid_label3 = valid_labels.iloc[:,2]

# get the fourth label of the valid dataset
valid_label4 = valid_labels.iloc[:,3]



# separate features and labels in test dataset
test_features = test_data.iloc[:, :-4]
test_labels = test_data.iloc[:, -4:]

# get the first label of the test dataset
test_label1 = test_labels.iloc[:,0]

# get the second label of the test dataset
test_label2 = test_labels.iloc[:,1]

# get the third label of the test dataset
test_label3 = test_labels.iloc[:,2]

# get the fourth label of the test dataset
test_label4 = test_labels.iloc[:,3]

#save variables
train_features_original = train_features.copy()
train_labels_original = train_labels.copy()
valid_features_original = valid_features.copy()
valid_labels_original = valid_labels.copy()
test_features_original = test_features.copy()
test_labels_original = test_labels.copy()

"""# Predicting Label 1 without Feature Engineering"""

# copy features and labels in train dataset
train_features = train_features_original.copy()
train_labels = train_labels_original.copy()
train_features_copy = train_features.copy()
train_labels_copy = train_labels.copy()

#copy of the first label of the train dataset
train_label1_copy = train_label1.copy()


# copy features and labels in valid dataset
valid_features = valid_features_original.copy()
valid_labels = valid_labels_original.copy()
valid_features_copy = valid_features.copy()
valid_labels_copy = valid_labels.copy()

#copy of the first label of the valid dataset
valid_label1_copy = valid_label1.copy()


# copy features and labels in test dataset
test_features = test_features_original.copy()
test_labels = test_labels_original.copy()
test_features_copy = test_features.copy()
test_labels_copy = test_labels.copy()

#copy of the first label of the test dataset
test_label1_copy = test_label1.copy()

# standardize the features
scaler = StandardScaler()
train_features_copy = scaler.fit_transform(train_features_copy)
valid_features_copy = scaler.transform(valid_features_copy)
test_features_copy = scaler.transform(test_features_copy)

#define model
base_model1 = SVC()

#train model
base_model1.fit(train_features_copy, train_label1_copy)

# predict on the train data
y_pred_base_train = base_model1.predict(train_features_copy)

# calculate evaluation metrics
accuracy = accuracy_score(train_label1_copy, y_pred_base_train)
precision = precision_score(train_label1_copy, y_pred_base_train, average='weighted' , zero_division=1)
recall = recall_score(train_label1_copy, y_pred_base_train, average='weighted')

print(f"Metrics for SVM on train data:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

# predict on the validation data
y_pred_base_valid = base_model1.predict(valid_features_copy)

# calculate evaluation metrics
accuracy = accuracy_score(valid_label1_copy, y_pred_base_valid)
precision = precision_score(valid_label1_copy, y_pred_base_valid, average='weighted', zero_division=1)
recall = recall_score(valid_label1_copy, y_pred_base_valid, average='weighted')

print(f"Metrics for SVM on valid data:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

# predict on the test data
y_pred_base_test = base_model1.predict(test_features_copy)

"""# Predicting Label 1 with Feature Engineering"""

# plotting the distribution of train_label1
labels, counts = np.unique(train_label1, return_counts=True)

plt.figure(figsize=(18, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.title('Distribution of Label 1')
plt.xlabel('Label 1')
plt.ylabel('Frequency')
plt.show()

#calculate the correlation matrix
correlation_matrix = train_features.corr()

mask = np.triu(np.ones_like(correlation_matrix))

# create a heatmap of the correlation matrix using seaborn
plt.figure(figsize=(12, 12))
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, mask=mask)
plt.title("Correlation Matrix")
plt.show()

# threshold for correlation
correlation_threshold = 0.9

highly_correlated = set()

# get highly correlated features
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:
            column_name = correlation_matrix.columns[i]
            highly_correlated.add(column_name)

print(highly_correlated)

# remove highly correlated features
train_features = train_features.drop(columns=highly_correlated)
valid_features = valid_features.drop(columns=highly_correlated)
test_features = test_features.drop(columns=highly_correlated)

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# set the correlation threshold
correlation_threshold = 0.05

# get the correlation matrix between features and train_label1
correlation_with_target = train_features.corrwith(train_label1)

# get the features that are above the correlation threshold
highly_correlated_features = correlation_with_target[correlation_with_target.abs() > correlation_threshold]

print(highly_correlated_features)

# drop the features with low correlated in train data
train_features = train_features[highly_correlated_features.index]

# drop the features with low correlated in valid data
valid_features = valid_features[highly_correlated_features.index]

# drop the features with low correlated in test data
test_features = test_features[highly_correlated_features.index]

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# standardize the features
scaler = StandardScaler()
standardized_train_features = scaler.fit_transform(train_features)
standardized_valid_features = scaler.transform(valid_features)
standardized_test_features = scaler.transform(test_features)

# threshold for variance
variance_threshold = 0.9

# apply PCA with the determined number of components
pca = PCA(n_components=variance_threshold, svd_solver='full')

pca_train_result = pca.fit_transform(standardized_train_features)
pca_valid_result = pca.transform(standardized_valid_features)
pca_test_result = pca.transform(standardized_test_features)

# explained variance ratio after dimensionality reduction
explained_variance_ratio_reduced = pca.explained_variance_ratio_
print("Explained Variance Ratio after Dimensionality Reduction:", explained_variance_ratio_reduced)

# plot explained variance ratio
plt.figure(figsize=(12, 6))
plt.bar(range(1, pca_train_result.shape[1] + 1), explained_variance_ratio_reduced)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio per Principal Component (Reduced)')
plt.show()

# show the reduced train feature matrix
print("Reduced Train feature matrix shape: {}".format(pca_train_result.shape))
# show the reduced valid feature matrix
print("Reduced valid feature matrix shape: {}".format(pca_valid_result.shape))
# show the reduced test feature matrix
print("Reduced test feature matrix shape: {}".format(pca_test_result.shape))

# define a list of classification models
classification_models = [
    ('Decision Tree', DecisionTreeClassifier()),
    ('K Neighbors', KNeighborsClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('SVM', SVC())
]

# number of features used in PCA
num_features = pca_train_result.shape[1]
print(f"Number of features: {num_features}\n")

# train and evaluate each classification model
for model_name, model in classification_models:
    # train the model on the training data
    model.fit(pca_train_result, train_label1)

    # predict on the train data
    y_pred_train = model.predict(pca_train_result)

    # calculate evaluation metrics
    accuracy = accuracy_score(train_label1, y_pred_train)
    precision = precision_score(train_label1, y_pred_train, average='weighted' , zero_division=1)
    recall = recall_score(train_label1, y_pred_train, average='weighted')

    print(f"Metrics for {model_name} on train data:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print("\n")

    # predict on the validation data
    y_pred_valid = model.predict(pca_valid_result)

    # calculate evaluation metrics
    accuracy = accuracy_score(valid_label1, y_pred_valid)
    precision = precision_score(valid_label1, y_pred_valid, average='weighted', zero_division=1)
    recall = recall_score(valid_label1, y_pred_valid, average='weighted')

    print(f"Metrics for {model_name} on validation data:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print("\n")

    # predict on the test data
    y_pred_test = model.predict(pca_test_result)

#save results of label 1 with feature engineering
pca_test_result_label1, y_pred_base_test_label1, y_pred_test_label1 = pca_test_result, y_pred_base_test, y_pred_test

"""# Predicting Label 2 without Feature Engineering"""

# copy features and labels in train dataset
train_features = train_features_original.copy()
train_labels = train_labels_original.copy()
train_features_copy = train_features_original.copy()
train_labels_copy = train_labels_original.copy()

# copy of the second label of the train dataset
train_label2_copy = train_label2.copy()


# copy features and labels in valid dataset
valid_features = valid_features_original.copy()
valid_labels = valid_labels_original.copy()
valid_features_copy = valid_features_original.copy()
valid_labels_copy = valid_labels_original.copy()

# copy of the second label of the valid dataset
valid_label2_copy = valid_label2.copy()


# copy features and labels in test dataset
test_features = test_features_original.copy()
test_labels = test_labels_original.copy()
test_features_copy = test_features_original.copy()
test_labels_copy = test_labels_original.copy()

# copy of the second label of the test dataset
test_label2_copy = test_label2.copy()

# standardize the features
scaler = StandardScaler()
train_features_copy = scaler.fit_transform(train_features_copy)
valid_features_copy = scaler.transform(valid_features_copy)
test_features_copy = scaler.transform(test_features_copy)

#define model
base_model2 = KNeighborsRegressor()

#train
base_model2.fit(train_features_copy, train_label2_copy)

# predict on the train data
y_pred_base_train = base_model2.predict(train_features_copy)

# calculate mean squred error
mse = mean_squared_error(train_label2_copy, y_pred_base_train)
r2s = r2_score(train_label2_copy, y_pred_base_train)

print(f"Metrics for KNeighborsRegressor on train data:")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R2 Score: {r2s:.2f}")
print("\n")

# predict on the validation data
y_pred_base_valid = base_model2.predict(valid_features_copy)

# calculate mean squred error
mse = mean_squared_error(valid_label2_copy, y_pred_base_valid)
r2s = r2_score(valid_label2_copy, y_pred_base_valid)

print(f"Metrics for KNeighborsRegressor on valid data:")
print(f"Mean Squared Error: {mse:.2f}")
print(f"R2 Score: {r2s:.2f}")
print("\n")

# predict on the test data
y_pred_base_test = base_model2.predict(test_features_copy)

"""# Predicting Label 2 with Feature Engineering"""

# plotting the distribution of train_label2
labels, counts = np.unique(train_label2, return_counts=True)

plt.figure(figsize=(18, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.title('Distribution of Label 2')
plt.xlabel('Label 2')
plt.ylabel('Frequency')
plt.show()

#calculate the correlation matrix
correlation_matrix = train_features.corr()

mask = np.triu(np.ones_like(correlation_matrix))

# create a heatmap of the correlation matrix using seaborn
plt.figure(figsize=(12, 12))
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, mask=mask)
plt.title("Correlation Matrix")
plt.show()

# threshold for correlation
correlation_threshold = 0.9

highly_correlated = set()

# get highly correlated features
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:
            column_name = correlation_matrix.columns[i]
            highly_correlated.add(column_name)

print(highly_correlated)

# remove highly correlated features
train_features = train_features.drop(columns=highly_correlated)
valid_features = valid_features.drop(columns=highly_correlated)
test_features = test_features.drop(columns=highly_correlated)

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# set the correlation threshold
correlation_threshold = 0.05

# get the correlation matrix between features and train_label2
correlation_with_target = train_features.corrwith(train_label2)

# get the features that are above the correlation threshold
highly_correlated_features = correlation_with_target[correlation_with_target.abs() > correlation_threshold]

print(highly_correlated_features)

# drop the features with low correlated in train data
train_features = train_features[highly_correlated_features.index]

# drop the features with low correlated in valid data
valid_features = valid_features[highly_correlated_features.index]

# drop the features with low correlated in test data
test_features = test_features[highly_correlated_features.index]

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# standardize the features
scaler = StandardScaler()
standardized_train_features = scaler.fit_transform(train_features)
standardized_valid_features = scaler.transform(valid_features)
standardized_test_features = scaler.transform(test_features)

# threshold for variance
variance_threshold = 0.9

# apply PCA with the determined number of components
pca = PCA(n_components=variance_threshold, svd_solver='full')

pca_train_result = pca.fit_transform(standardized_train_features)
pca_valid_result = pca.transform(standardized_valid_features)
pca_test_result = pca.transform(standardized_test_features)

# explained variance ratio after dimensionality reduction
explained_variance_ratio_reduced = pca.explained_variance_ratio_
print("Explained Variance Ratio after Dimensionality Reduction:", explained_variance_ratio_reduced)

# plot explained variance ratio
plt.figure(figsize=(12, 6))
plt.bar(range(1, pca_train_result.shape[1] + 1), explained_variance_ratio_reduced)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio per Principal Component (Reduced)')
plt.show()

# show the reduced train feature matrix
print("Reduced train feature matrix shape: {}".format(pca_train_result.shape))
# show the reduced valid feature matrix
print("Reduced valid feature matrix shape: {}".format(pca_valid_result.shape))
# show the reduced test feature matrix
print("Reduced test feature matrix shape: {}".format(pca_test_result.shape))

# define a list of regression models
regression_models = [
    ('Decision Tree', DecisionTreeRegressor()),
    ('K Neighbors', KNeighborsRegressor()),
    ('Linear Regression', LinearRegression()),
    ('Random Forest', RandomForestRegressor()),
    ('XGBoost', XGBRegressor())
]

# number of features used in PCA
num_features = pca_train_result.shape[1]
print(f"Number of features: {num_features}\n")

# train and evaluate each regression model
for model_name, model in regression_models:
    # train the model on the training data
    model.fit(pca_train_result, train_label2)

    # predict on the train data
    y_pred_train = model.predict(pca_train_result)

    # calculate mean squred error
    mse = mean_squared_error(train_label2, y_pred_train)
    r2s = r2_score(train_label2, y_pred_train)

    print(f"Metrics for {model_name} on train data:")
    print(f"Mean Squared Error: {mse:.2f}")
    print(f"R2 Score: {r2s:.2f}")
    print("\n")

    # predict on the validation data
    y_pred_valid = model.predict(pca_valid_result)

    # calculate mean squred error
    mse = mean_squared_error(valid_label2, y_pred_valid)
    r2s = r2_score(valid_label2, y_pred_valid)

    print(f"Metrics for {model_name} on validation data:")
    print(f"Mean Squared Error: {mse:.2f}")
    print(f"R2 Score: {r2s:.2f}")
    print("\n")

    # predict on the test data
    y_pred_test = model.predict(pca_test_result)

#save results of label 2 with feature engineering
pca_test_result_label2, y_pred_base_test_label2, y_pred_test_label2 = pca_test_result, y_pred_base_test, y_pred_test

"""# Predicting Label 3 without Feature Engineering"""

# copy features and labels in train dataset
train_features = train_features_original.copy()
train_labels = train_labels_original.copy()
train_features_copy = train_features_original.copy()
train_labels_copy = train_labels_original.copy()

# copy of the third label of the train dataset
train_label3_copy = train_label3.copy()


# copy features and labels in valid dataset
valid_features = valid_features_original.copy()
valid_labels = valid_labels_original.copy()

valid_features_copy = valid_features_original.copy()
valid_labels_copy = valid_labels_original.copy()

# copy of the third label of the valid dataset
valid_label3_copy = valid_label3.copy()


# copy features and labels in test dataset
test_features = test_features_original.copy()
test_labels = test_labels_original.copy()
test_features_copy = test_features_original.copy()
test_labels_copy = test_labels_original.copy()

# copy of the third label of the test dataset
test_label3_copy = test_label3.copy()

# standardize the features
scaler = StandardScaler()
train_features_copy = scaler.fit_transform(train_features_copy)
valid_features_copy = scaler.transform(valid_features_copy)
test_features_copy = scaler.transform(test_features_copy)

#define model
base_model3 = KNeighborsClassifier()

#train model
base_model3.fit(train_features_copy, train_label3_copy)

# predict on the train data
y_pred_base_train = base_model3.predict(train_features_copy)

# calculate evaluation metrics
accuracy = accuracy_score(train_label3_copy, y_pred_base_train)
precision = precision_score(train_label3_copy, y_pred_base_train, average='weighted' , zero_division=1)
recall = recall_score(train_label3_copy, y_pred_base_train, average='weighted')

print(f"Metrics for KNN on train data:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

# predict on the validation data
y_pred_base_valid = base_model3.predict(valid_features_copy)

# calculate evaluation metrics
accuracy = accuracy_score(valid_label3_copy, y_pred_base_valid)
precision = precision_score(valid_label3_copy, y_pred_base_valid, average='weighted', zero_division=1)
recall = recall_score(valid_label3_copy, y_pred_base_valid, average='weighted')

print(f"Metrics for KNN on valid data:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

# predict on the test data
y_pred_base_test = base_model3.predict(test_features_copy)

"""# Predicting Label 3 with Feature Engineering"""

# plotting the distribution of train_label3
labels, counts = np.unique(train_label3, return_counts=True)

plt.figure(figsize=(18, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.title('Distribution of Label 3')
plt.xlabel('Label 3')
plt.ylabel('Frequency')
plt.show()

#calculate the correlation matrix
correlation_matrix = train_features.corr()

mask = np.triu(np.ones_like(correlation_matrix))

# create a heatmap of the correlation matrix using seaborn
plt.figure(figsize=(12, 12))
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, mask=mask)
plt.title("Correlation Matrix")
plt.show()

# threshold for correlation
correlation_threshold = 0.9

highly_correlated = set()

# get highly correlated features
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:
            column_name = correlation_matrix.columns[i]
            highly_correlated.add(column_name)

print(highly_correlated)

# remove highly correlated features
train_features = train_features.drop(columns=highly_correlated)
valid_features = valid_features.drop(columns=highly_correlated)
test_features = test_features.drop(columns=highly_correlated)

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# set the correlation threshold
correlation_threshold = 0.05

# get the correlation matrix between features and train_label3
correlation_with_target = train_features.corrwith(train_label3)

# get the features that are above the correlation threshold
highly_correlated_features = correlation_with_target[correlation_with_target.abs() > correlation_threshold]

print(highly_correlated_features)

# drop the features with low correlated in train data
train_features = train_features[highly_correlated_features.index]

# drop the features with low correlated in valid data
valid_features = valid_features[highly_correlated_features.index]

# drop the features with low correlated in test data
test_features = test_features[highly_correlated_features.index]

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# standardize the features
scaler = StandardScaler()
standardized_train_features = scaler.fit_transform(train_features)
standardized_valid_features = scaler.transform(valid_features)
standardized_test_features = scaler.transform(test_features)

# threshold for variance
variance_threshold = 0.9

# apply PCA with the determined number of components
pca = PCA(n_components=variance_threshold, svd_solver='full')

pca_train_result = pca.fit_transform(standardized_train_features)
pca_valid_result = pca.transform(standardized_valid_features)
pca_test_result = pca.transform(standardized_test_features)

# explained variance ratio after dimensionality reduction
explained_variance_ratio_reduced = pca.explained_variance_ratio_
print("Explained Variance Ratio after Dimensionality Reduction:", explained_variance_ratio_reduced)

# plot explained variance ratio
plt.figure(figsize=(18, 10))
plt.bar(range(1, pca_train_result.shape[1] + 1), explained_variance_ratio_reduced)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio per Principal Component (Reduced)')
plt.show()

# show the reduced train feature matrix
print("Reduced Train feature matrix shape: {}".format(pca_train_result.shape))
# show the reduced valid feature matrix
print("Reduced valid feature matrix shape: {}".format(pca_valid_result.shape))
# show the reduced test feature matrix
print("Reduced test feature matrix shape: {}".format(pca_test_result.shape))

# define a list of classification models
classification_models = [
    ('Decision Tree', DecisionTreeClassifier()),
    ('K Neighbors', KNeighborsClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('SVM', SVC()),
    ('XGBoost', XGBClassifier()),
]

# number of features used in PCA
num_features = pca_train_result.shape[1]
print(f"Number of features: {num_features}\n")

# train and evaluate each classification model
for model_name, model in classification_models:
    # train the model on the training data
    model.fit(pca_train_result, train_label3)

    # predict on the train data
    y_pred_train = model.predict(pca_train_result)

    # calculate evaluation metrics
    accuracy = accuracy_score(train_label3, y_pred_train)
    precision = precision_score(train_label3, y_pred_train, average='weighted' , zero_division=1)
    recall = recall_score(train_label3, y_pred_train, average='weighted')

    print(f"Metrics for {model_name} on train data:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print("\n")

    # predict on the validation data
    y_pred_valid = model.predict(pca_valid_result)

    # calculate evaluation metrics
    accuracy = accuracy_score(valid_label3, y_pred_valid)
    precision = precision_score(valid_label3, y_pred_valid, average='weighted', zero_division=1)
    recall = recall_score(valid_label3, y_pred_valid, average='weighted')

    print(f"Metrics for {model_name} on validation data:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print("\n")

    # predict on the test data
    y_pred_test = model.predict(pca_test_result)

#save results of label 3 with feature engineering
pca_test_result_label3, y_pred_base_test_label3, y_pred_test_label3 = pca_test_result, y_pred_base_test, y_pred_test

"""# Predicting Label 4 without Feature Engineering

"""

# copy features and labels in train dataset
train_features = train_features_original.copy()
train_labels = train_labels_original.copy()
train_features_copy = train_features_original.copy()
train_labels_copy = train_labels_original.copy()

# copy of the fourth label of the train dataset
train_label4_copy = train_label4.copy()


# copy features and labels in valid dataset
valid_features = valid_features_original.copy()
valid_labels = valid_labels_original.copy()
valid_features_copy = valid_features_original.copy()
valid_labels_copy = valid_labels_original.copy()

# copy of the fourth label of the valid dataset
valid_label4_copy = valid_label4.copy()



# copy features and labels in test dataset
test_features = test_features_original.copy()
test_labels = test_labels_original.copy()
test_features_copy = test_features_original.copy()
test_labels_copy = test_labels_original.copy()

# copy of the fourth label of the test dataset
test_label4_copy = test_label4.copy()

# standardize the features
scaler = StandardScaler()
train_features_copy = scaler.fit_transform(train_features_copy)
valid_features_copy = scaler.transform(valid_features_copy)
test_features_copy = scaler.transform(test_features_copy)

#define model
base_model3 = KNeighborsClassifier()

#train model
base_model3.fit(train_features_copy, train_label4_copy)

# predict on the train data
y_pred_base_train = base_model3.predict(train_features_copy)

# calculate evaluation metrics
accuracy = accuracy_score(train_label4_copy, y_pred_base_train)
precision = precision_score(train_label4_copy, y_pred_base_train, average='weighted' , zero_division=1)
recall = recall_score(train_label4_copy, y_pred_base_train, average='weighted')

print(f"Metrics for KNN on train data:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

# predict on the validation data
y_pred_base_valid = base_model3.predict(valid_features_copy)

# calculate evaluation metrics
accuracy = accuracy_score(valid_label4_copy, y_pred_base_valid)
precision = precision_score(valid_label4_copy, y_pred_base_valid, average='weighted', zero_division=1)
recall = recall_score(valid_label4_copy, y_pred_base_valid, average='weighted')

print(f"Metrics for KNN on valid data:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print("\n")

# predict on the test data
y_pred_base_test = base_model3.predict(test_features_copy)

"""# Predicting Label 4 with Feature Engineering"""

# plotting the distribution of train_label4
labels, counts = np.unique(train_label4, return_counts=True)

plt.figure(figsize=(18, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.title('Distribution of Label 4')
plt.xlabel('Label 4')
plt.ylabel('Frequency')
plt.show()

#calculate the correlation matrix
correlation_matrix = train_features.corr()

mask = np.triu(np.ones_like(correlation_matrix))

# create a heatmap of the correlation matrix using seaborn
plt.figure(figsize=(12, 12))
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, mask=mask)
plt.title("Correlation Matrix")
plt.show()

# threshold for correlation
correlation_threshold = 0.9

highly_correlated = set()

# get highly correlated features
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:
            column_name = correlation_matrix.columns[i]
            highly_correlated.add(column_name)

print(highly_correlated)

# remove highly correlated features
train_features = train_features.drop(columns=highly_correlated)
valid_features = valid_features.drop(columns=highly_correlated)
test_features = test_features.drop(columns=highly_correlated)

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# set the correlation threshold
correlation_threshold = 0.05

# get the correlation matrix between features and train_label4
correlation_with_target = train_features.corrwith(train_label4)

# get the features that are above the correlation threshold
highly_correlated_features = correlation_with_target[correlation_with_target.abs() > correlation_threshold]

print(highly_correlated_features)

# drop the features with low correlated in train data
train_features = train_features[highly_correlated_features.index]

# drop the features with low correlated in valid data
valid_features = valid_features[highly_correlated_features.index]

# drop the features with low correlated in test data
test_features = test_features[highly_correlated_features.index]

# show the filtered train feature count
print("Filtered train features: {}".format(train_features.shape))

# show the filtered valid feature count
print("Filtered valid features: {}".format(valid_features.shape))

# show the filtered test feature count
print("Filtered test features: {}".format(test_features.shape))

# standardize the features
scaler = StandardScaler()
standardized_train_features = scaler.fit_transform(train_features)
standardized_valid_features = scaler.transform(valid_features)
standardized_test_features = scaler.transform(test_features)

# threshold for variance
variance_threshold = 0.9

# apply PCA with the determined number of components
pca = PCA(n_components=variance_threshold, svd_solver='full')

pca_train_result = pca.fit_transform(standardized_train_features)
pca_valid_result = pca.transform(standardized_valid_features)
pca_test_result = pca.transform(standardized_test_features)

# explained variance ratio after dimensionality reduction
explained_variance_ratio_reduced = pca.explained_variance_ratio_
print("Explained Variance Ratio after Dimensionality Reduction:", explained_variance_ratio_reduced)

# plot explained variance ratio
plt.figure(figsize=(12, 6))
plt.bar(range(1, pca_train_result.shape[1] + 1), explained_variance_ratio_reduced)
plt.title('Explained Variance Ratio per Principal Component (Reduced)')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.show()

# show the reduced train feature matrix
print("Reduced Train feature matrix shape: {}".format(pca_train_result.shape))
# show the reduced valid feature matrix
print("Reduced valid feature matrix shape: {}".format(pca_valid_result.shape))
# show the reduced test feature matrix
print("Reduced test feature matrix shape: {}".format(pca_test_result.shape))

# define a list of classification models
classification_models = [
    ('Decision Tree', DecisionTreeClassifier()),
    ('K Neighbors', KNeighborsClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('SVM', SVC()),
    ('XGBoost', XGBClassifier()),
]

# number of features used in PCA
num_features = pca_train_result.shape[1]
print(f"Number of features: {num_features}\n")

# train and evaluate each classification model
for model_name, model in classification_models:
    # train the model on the training data
    model.fit(pca_train_result, train_label4)

    # predict on the train data
    y_pred_train = model.predict(pca_train_result)

    # calculate evaluation metrics
    accuracy = accuracy_score(train_label4, y_pred_train)
    precision = precision_score(train_label4, y_pred_train, average='weighted' , zero_division=1)
    recall = recall_score(train_label4, y_pred_train, average='weighted')

    print(f"Metrics for {model_name} on train data:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print("\n")

    # predict on the validation data
    y_pred_valid = model.predict(pca_valid_result)

    # calculate evaluation metrics
    accuracy = accuracy_score(valid_label4, y_pred_valid)
    precision = precision_score(valid_label4, y_pred_valid, average='weighted', zero_division=1)
    recall = recall_score(valid_label4, y_pred_valid, average='weighted')

    print(f"Metrics for {model_name} on validation data:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print("\n")

    # predict on the test data
    y_pred_test = model.predict(pca_test_result)

#save results of label 4 with feature engineering
pca_test_result_label4, y_pred_base_test_label4, y_pred_test_label4 = pca_test_result, y_pred_base_test, y_pred_test

"""# Generate Output CSV"""

destination = '/content/drive/MyDrive/Semester 7/3 - CS4622 - Machine Learning/Lab 1/190495D_'

# create the dataframe and save it as a csv file
def create_csv(features, pred_before_fe, pred_after_fe, filename):
  feature_count = features.shape[1]

  header_row = [f"new_feature_{i}" for i in range(1,feature_count+1)]

  df = pd.DataFrame(features, columns  = header_row)

  df.insert(loc=0, column='Predicted labels before feature engineering', value=pred_before_fe)
  df.insert(loc=1, column='Predicted labels after feature engineering', value=pred_after_fe)
  df.insert(loc=2, column='No of new features', value=np.repeat(feature_count, features.shape[0]))

  df.to_csv(destination+filename, index=False)

# create the csv output file for label 1
create_csv(pca_test_result_label1, y_pred_base_test_label1, y_pred_test_label1, 'label_1.csv')

# create the csv output file for label 2
create_csv(pca_test_result_label2, y_pred_base_test_label2, y_pred_test_label2, 'label_2.csv')

# create the csv output file for label 3
create_csv(pca_test_result_label3, y_pred_base_test_label3, y_pred_test_label3, 'label_3.csv')

# create the csv output file for label 4
create_csv(pca_test_result_label4, y_pred_base_test_label4, y_pred_test_label4, 'label_4.csv')